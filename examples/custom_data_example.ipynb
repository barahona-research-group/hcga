{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is generating some synthetic graph dataset with node features. \n",
    "\n",
    "\n",
    "#defining limits on number of nodes\n",
    "n_min = 20\n",
    "n_max = 50\n",
    "\n",
    "#number of graphs\n",
    "num_g = 100\n",
    "\n",
    "# number of node features - in this example I will generate random node features that aren't useful for classifcation\n",
    "n_nf = 3\n",
    "\n",
    "\n",
    "# empty list of graphs and labels\n",
    "graphs = []\n",
    "labels = []\n",
    "node_features = []\n",
    "\n",
    "\n",
    "# setting limits on probability of edge existing for random graphs\n",
    "p_min = 0.1\n",
    "p_max = 0.5\n",
    "\n",
    "# adding 50 random graphs (label 0)\n",
    "for i in range(int(num_g/2)):\n",
    "    rand_n = np.random.randint(n_min,n_max)\n",
    "    rand_p = np.random.randint(int(p_min*100),int(p_max*100))/100   \n",
    "    \n",
    "    g = nx.fast_gnp_random_graph(rand_n,rand_p)    \n",
    "    node_feat_matrix = np.random.random((rand_n,n_nf))\n",
    "    \n",
    "    graphs.append(nx.to_numpy_array(g))\n",
    "    node_features.append(node_feat_matrix)\n",
    "    \n",
    "    labels.append(0)\n",
    "\n",
    "# setting limits on number of edges to add per node\n",
    "m_min = 1\n",
    "m_max = 5\n",
    "\n",
    "# adding 50  powerlaw cluster graphs (label 1)\n",
    "for i in range(int(num_g/2)):\n",
    "    rand_n = np.random.randint(n_min,n_max)\n",
    "    rand_p = np.random.randint(int(p_min*100),int(p_max*100))/100   \n",
    "    rand_m = np.random.randint(m_min,m_max)\n",
    "    \n",
    "    g = nx.powerlaw_cluster_graph(rand_n, rand_m, rand_p)\n",
    "    node_feat_matrix = np.random.random((rand_n,n_nf))\n",
    "    \n",
    "    graphs.append(nx.to_numpy_array(g))\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "    labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have three lists of length 100. The graphs list is composed of numpy arrays that represent the adjacency matrix of the graph. The node features list is composed of numpy arrays that contain the node information for each graph. The labels list is a list of integers that corresponds to the class label for each graph.\n",
    "\n",
    "The next step is to take this data and convert it into an appropriate format for hcga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting this data into the format required for hcga\n",
    "\n",
    "from hcga.graph import Graph, GraphCollection\n",
    "\n",
    "# create graph collection object\n",
    "g_c = GraphCollection()\n",
    "\n",
    "\n",
    "# looping over each graph and appending it to the graph collection object\n",
    "for i,A in enumerate(graphs):\n",
    "    \n",
    "    # generating a sparse matrix\n",
    "    sA = sc.sparse.coo_matrix(A)\n",
    "    \n",
    "    # extracting edge list from scipy sparse matrix\n",
    "    edges = np.array([sA.row,sA.col,sA.data]).T\n",
    "    \n",
    "    # passing edge list to pandas dataframe\n",
    "    edges_df = pd.DataFrame(edges, columns = ['start_node', 'end_node', 'weight'])\n",
    "\n",
    "    # creating node ids based on size of adjancency matrix\n",
    "    nodes = np.arange(0,A.shape[0])\n",
    "    \n",
    "    # loading node ids into dataframe\n",
    "    nodes_df = pd.DataFrame(index=nodes)\n",
    "\n",
    "    # each node should have the same number of node features across all graphs\n",
    "    # converting node features array to list such that each node is assigned a list.\n",
    "    nodes_df['attributes'] = node_features[i].tolist()\n",
    "\n",
    "    # extracting graph label from labels\n",
    "    graph_label = labels[i]\n",
    "    \n",
    "    # create a single graph object\n",
    "    graph = Graph(nodes_df, edges_df, graph_label)\n",
    "\n",
    "    # add new graph to the collection\n",
    "    g_c.add_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform some sanity checks\n",
    "\n",
    "print('There are {} graphs'.format(len(g_c.graphs)))\n",
    "print('There are {} features per node'.format(g_c.get_n_node_features()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can save this if we want to and run everything from the command line\n",
    "from hcga.io import save_dataset\n",
    "\n",
    "save_dataset(g_c, 'custom_dataset', folder='./datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features\n",
    "\n",
    "We have now produced a pickle dataset of your own custom data. We can now run the feature extraction from the command line using the following commands:\n",
    "\n",
    "hcga extract_features ./datasets/custom_dataset.pkl -m fast -n 4 -sl advanced --timeout 10 \n",
    "\n",
    "\n",
    "Alternatively,we could import the Hcga class and run the feature extraction and analysis from within the notebook. We will do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hcga.io import load_dataset\n",
    "\n",
    "graphs = load_dataset('./datasets/custom_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import hcga object\n",
    "from hcga import Hcga\n",
    "\n",
    "# define an object\n",
    "h = Hcga()\n",
    "\n",
    "#assigning the graphs field to the recently created dataset\n",
    "h.graphs = graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extracting all features here\n",
    "h.extract(mode='fast')\n",
    "\n",
    "# saving all features into a pickle\n",
    "h.save_features('./results/custom_dataset_results.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved features\n",
    "h.load_features('./results/custom_dataset_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# implement a classification analyse of the features\n",
    "h.analyse_features(feature_file='./results/custom_dataset_results.pkl',results_folder='./results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
