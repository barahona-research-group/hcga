{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Classification with Synthetic data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "if not Path(\"datasets\").exists():\n",
    "    os.mkdir(\"datasets\")\n",
    "if not Path(\"results\").exists():\n",
    "    os.mkdir(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is generating some synthetic graph dataset with node features.\n",
    "\n",
    "\n",
    "# defining limits on number of nodes\n",
    "n_min = 20\n",
    "n_max = 50\n",
    "\n",
    "# number of graphs\n",
    "num_g = 100\n",
    "\n",
    "# number of node features - in this example I will generate random node features that aren't useful for classifcation\n",
    "n_nf = 3\n",
    "\n",
    "\n",
    "# empty list of graphs and labels\n",
    "graphs = []\n",
    "labels = []\n",
    "node_features = []\n",
    "\n",
    "\n",
    "# setting limits on probability of edge existing for random graphs\n",
    "p_min = 0.1\n",
    "p_max = 0.5\n",
    "\n",
    "# adding 50 random graphs (label 0)\n",
    "for i in range(int(num_g / 2)):\n",
    "    rand_n = np.random.randint(n_min, n_max)\n",
    "    rand_p = np.random.randint(int(p_min * 100), int(p_max * 100)) / 100\n",
    "\n",
    "    g = nx.fast_gnp_random_graph(rand_n, rand_p)\n",
    "    node_feat_matrix = np.random.random((rand_n, n_nf))\n",
    "\n",
    "    graphs.append(nx.to_numpy_array(g))\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "    labels.append(0)\n",
    "\n",
    "# setting limits on number of edges to add per node\n",
    "m_min = 1\n",
    "m_max = 5\n",
    "\n",
    "# adding 50  powerlaw cluster graphs (label 1)\n",
    "for i in range(int(num_g / 2)):\n",
    "    rand_n = np.random.randint(n_min, n_max)\n",
    "    rand_p = np.random.randint(int(p_min * 100), int(p_max * 100)) / 100\n",
    "    rand_m = np.random.randint(m_min, m_max)\n",
    "\n",
    "    g = nx.powerlaw_cluster_graph(rand_n, rand_m, rand_p)\n",
    "    node_feat_matrix = np.random.random((rand_n, n_nf))\n",
    "\n",
    "    graphs.append(nx.to_numpy_array(g))\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "    labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load graphs into graph object\n",
    "\n",
    "We now have three lists of length 100. The graphs list is composed of numpy arrays that represent the adjacency matrix of the graph. The node features list is composed of numpy arrays that contain the node information for each graph. The labels list is a list of integers that corresponds to the class label for each graph.\n",
    "\n",
    "The next step is to take this data and convert it into an appropriate format for hcga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting this data into the format required for hcga\n",
    "\n",
    "from hcga.graph import Graph, GraphCollection\n",
    "\n",
    "# create graph collection object\n",
    "g_c = GraphCollection()\n",
    "\n",
    "# add graphs, node features and labels to the object\n",
    "g_c.add_graph_list(graphs, node_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform some sanity checks\n",
    "\n",
    "print(\"There are {} graphs\".format(len(g_c.graphs)))\n",
    "print(\"There are {} features per node\".format(g_c.get_n_node_features()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can save this if we want to and run everything from the command line\n",
    "from hcga.io import save_dataset\n",
    "\n",
    "save_dataset(\n",
    "    g_c,\n",
    "    \"custom_dataset_classification\",\n",
    "    folder=\"./datasets/custom_dataset_classification\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features\n",
    "\n",
    "We have now produced a pickle dataset of your own custom data. We can now run the feature extraction from the command line using the following commands:\n",
    "\n",
    "hcga extract_features ./datasets/custom_dataset.pkl -m fast -n 4 -sl advanced --timeout 10 \n",
    "\n",
    "\n",
    "Alternatively,we could import the Hcga class and run the feature extraction and analysis from within the notebook. We will do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hcga object\n",
    "from hcga.hcga import Hcga\n",
    "\n",
    "# define an object\n",
    "h = Hcga()\n",
    "\n",
    "# load previously saved dataset\n",
    "h.load_data(\n",
    "    \"./datasets/custom_dataset_classification/custom_dataset_classification.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extracting all features here\n",
    "h.extract(mode=\"fast\", n_workers=4, timeout=20)\n",
    "\n",
    "# saving all features into a pickle\n",
    "h.save_features(\"./results/custom_dataset_classification/all_features.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved features\n",
    "\n",
    "h.load_features(\"./results/custom_dataset_classification/all_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a classification analyse of the features\n",
    "\n",
    "h.analyse_features(\n",
    "    feature_file=\"./results/custom_dataset_classification/all_features.pkl\",\n",
    "    results_folder=\"./results/custom_dataset_classification\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
